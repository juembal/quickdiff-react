{"ast":null,"code":"/**\r\n * LLM Service for QuickDiff React App\r\n * Supports multiple LLM providers: OpenAI, Anthropic, Ollama (local), and Gemini\r\n */\n\nexport class LLMService {\n  constructor() {\n    this.config = {\n      provider: 'openai',\n      // 'openai', 'anthropic', 'ollama', 'gemini'\n      apiKey: '',\n      // Set via environment or user input\n      model: 'gpt-3.5-turbo',\n      // Default model\n      baseUrl: 'https://api.openai.com/v1',\n      // Can be changed for local/custom endpoints\n      maxTokens: 1000,\n      temperature: 0.7\n    };\n\n    // Load config from localStorage if available\n    this.loadConfig();\n  }\n  loadConfig() {\n    try {\n      const savedConfig = localStorage.getItem('quickdiff_llm_config');\n      if (savedConfig) {\n        this.config = {\n          ...this.config,\n          ...JSON.parse(savedConfig)\n        };\n      }\n    } catch (error) {\n      console.error('Error loading LLM config:', error);\n    }\n  }\n  saveConfig() {\n    try {\n      // Don't save API key to localStorage for security\n      const configToSave = {\n        ...this.config\n      };\n      delete configToSave.apiKey;\n      localStorage.setItem('quickdiff_llm_config', JSON.stringify(configToSave));\n    } catch (error) {\n      console.error('Error saving LLM config:', error);\n    }\n  }\n  updateConfig(newConfig) {\n    this.config = {\n      ...this.config,\n      ...newConfig\n    };\n    this.saveConfig();\n  }\n  getConfig() {\n    return {\n      ...this.config\n    };\n  }\n\n  // Check if LLM is properly configured\n  isConfigured() {\n    if (this.config.provider === 'ollama') {\n      return true; // Ollama doesn't require API key\n    }\n    return this.config.apiKey && this.config.apiKey.trim().length > 0;\n  }\n\n  // Generate analysis using the configured LLM\n  async generateAnalysis(type, originalText, changedText) {\n    if (!this.isConfigured()) {\n      throw new Error('LLM not configured. Please set up your API key and provider.');\n    }\n    const prompt = this.createPrompt(type, originalText, changedText);\n    try {\n      const response = await this.callLLM(prompt);\n      return this.formatResponse(type, response);\n    } catch (error) {\n      console.error('LLM API Error:', error);\n      throw new Error(`Failed to generate ${type} analysis: ${error.message}`);\n    }\n  }\n\n  // Create appropriate prompt based on analysis type\n  createPrompt(type, originalText, changedText) {\n    const baseContext = `You are analyzing text differences. Here are the texts:\n\nORIGINAL TEXT:\n${originalText || '(empty)'}\n\nCHANGED TEXT:\n${changedText || '(empty)'}\n\n`;\n    switch (type) {\n      case 'explain':\n        return baseContext + `Please provide a detailed explanation of the differences between these texts. Focus on:\n1. What specific changes were made\n2. The significance of these changes\n3. Potential impact or implications\n4. Key statistics (word count, line count changes)\n\nFormat your response in HTML with appropriate headings and structure.`;\n      case 'rewrite':\n        return baseContext + `Please provide specific rewrite suggestions to improve the text. Focus on:\n1. Style and clarity improvements\n2. Grammar and syntax corrections\n3. Structure and organization suggestions\n4. Tone and readability enhancements\n\nFormat your response in HTML with specific, actionable suggestions.`;\n      case 'summary':\n        return baseContext + `Please provide a concise summary of both texts, highlighting:\n1. Main topics and themes\n2. Key differences between versions\n3. Important information preserved or lost\n4. Overall content assessment\n\nFormat your response in HTML with clear sections.`;\n      case 'tone':\n        return baseContext + `Please analyze the tone and style of both texts. Include:\n1. Sentiment analysis (positive, negative, neutral)\n2. Formality level assessment\n3. Writing style characteristics\n4. Tone changes between versions\n5. Recommendations for tone consistency\n\nFormat your response in HTML with detailed analysis.`;\n      case 'cleanup':\n        return baseContext + `Please analyze the text for formatting and cleanup issues. Identify:\n1. Formatting inconsistencies\n2. Punctuation and spacing issues\n3. Structural problems\n4. Specific cleanup recommendations\n5. Best practices for improvement\n\nFormat your response in HTML with actionable cleanup suggestions.`;\n      default:\n        return baseContext + `Please analyze these texts and provide insights about their differences and characteristics. Format your response in HTML.`;\n    }\n  }\n\n  // Call the appropriate LLM API based on provider\n  async callLLM(prompt) {\n    switch (this.config.provider) {\n      case 'openai':\n        return await this.callOpenAI(prompt);\n      case 'anthropic':\n        return await this.callAnthropic(prompt);\n      case 'ollama':\n        return await this.callOllama(prompt);\n      case 'gemini':\n        return await this.callGemini(prompt);\n      default:\n        throw new Error(`Unsupported LLM provider: ${this.config.provider}`);\n    }\n  }\n\n  // OpenAI API call\n  async callOpenAI(prompt) {\n    const response = await fetch(`${this.config.baseUrl}/chat/completions`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'Authorization': `Bearer ${this.config.apiKey}`\n      },\n      body: JSON.stringify({\n        model: this.config.model,\n        messages: [{\n          role: 'system',\n          content: 'You are a helpful assistant that analyzes text differences and provides detailed insights.'\n        }, {\n          role: 'user',\n          content: prompt\n        }],\n        max_tokens: this.config.maxTokens,\n        temperature: this.config.temperature\n      })\n    });\n    if (!response.ok) {\n      var _error$error;\n      const error = await response.json();\n      throw new Error(((_error$error = error.error) === null || _error$error === void 0 ? void 0 : _error$error.message) || `HTTP ${response.status}`);\n    }\n    const data = await response.json();\n    return data.choices[0].message.content;\n  }\n\n  // Anthropic Claude API call\n  async callAnthropic(prompt) {\n    const response = await fetch('https://api.anthropic.com/v1/messages', {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json',\n        'x-api-key': this.config.apiKey,\n        'anthropic-version': '2023-06-01'\n      },\n      body: JSON.stringify({\n        model: this.config.model || 'claude-3-sonnet-20240229',\n        max_tokens: this.config.maxTokens,\n        messages: [{\n          role: 'user',\n          content: prompt\n        }]\n      })\n    });\n    if (!response.ok) {\n      var _error$error2;\n      const error = await response.json();\n      throw new Error(((_error$error2 = error.error) === null || _error$error2 === void 0 ? void 0 : _error$error2.message) || `HTTP ${response.status}`);\n    }\n    const data = await response.json();\n    return data.content[0].text;\n  }\n\n  // Ollama local API call\n  async callOllama(prompt) {\n    const ollamaUrl = this.config.baseUrl || 'http://localhost:11434';\n    const response = await fetch(`${ollamaUrl}/api/generate`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        model: this.config.model || 'llama2',\n        prompt: prompt,\n        stream: false,\n        options: {\n          temperature: this.config.temperature,\n          num_predict: this.config.maxTokens\n        }\n      })\n    });\n    if (!response.ok) {\n      throw new Error(`Ollama API error: HTTP ${response.status}`);\n    }\n    const data = await response.json();\n    return data.response;\n  }\n\n  // Google Gemini API call\n  async callGemini(prompt) {\n    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${this.config.model || 'gemini-pro'}:generateContent?key=${this.config.apiKey}`, {\n      method: 'POST',\n      headers: {\n        'Content-Type': 'application/json'\n      },\n      body: JSON.stringify({\n        contents: [{\n          parts: [{\n            text: prompt\n          }]\n        }],\n        generationConfig: {\n          temperature: this.config.temperature,\n          maxOutputTokens: this.config.maxTokens\n        }\n      })\n    });\n    if (!response.ok) {\n      var _error$error3;\n      const error = await response.json();\n      throw new Error(((_error$error3 = error.error) === null || _error$error3 === void 0 ? void 0 : _error$error3.message) || `HTTP ${response.status}`);\n    }\n    const data = await response.json();\n    return data.candidates[0].content.parts[0].text;\n  }\n\n  // Format the LLM response into the expected structure\n  formatResponse(type, content) {\n    const typeIcons = {\n      explain: 'üß†',\n      rewrite: '‚ú®',\n      summary: 'üìù',\n      tone: 'üé≠',\n      cleanup: 'üßπ'\n    };\n    const typeTitles = {\n      explain: 'AI Explanation',\n      rewrite: 'AI Rewrite Suggestions',\n      summary: 'AI Summary',\n      tone: 'AI Tone Analysis',\n      cleanup: 'AI Text Cleanup'\n    };\n    return {\n      title: `${typeIcons[type] || 'ü§ñ'} ${typeTitles[type] || 'AI Analysis'}`,\n      content: `<div class=\"ai-${type}\">${content}</div>`,\n      type: type\n    };\n  }\n\n  // Test the LLM connection\n  async testConnection() {\n    try {\n      const testPrompt = \"Please respond with 'Connection successful!' to test the API.\";\n      const response = await this.callLLM(testPrompt);\n      return {\n        success: true,\n        response\n      };\n    } catch (error) {\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n\n  // Get available models for the current provider\n  getAvailableModels() {\n    switch (this.config.provider) {\n      case 'openai':\n        return ['gpt-4', 'gpt-4-turbo-preview', 'gpt-3.5-turbo', 'gpt-3.5-turbo-16k'];\n      case 'anthropic':\n        return ['claude-3-opus-20240229', 'claude-3-sonnet-20240229', 'claude-3-haiku-20240307'];\n      case 'ollama':\n        return ['llama2', 'llama2:13b', 'llama2:70b', 'codellama', 'mistral', 'neural-chat'];\n      case 'gemini':\n        return ['gemini-pro', 'gemini-pro-vision'];\n      default:\n        return [];\n    }\n  }\n}","map":{"version":3,"names":["LLMService","constructor","config","provider","apiKey","model","baseUrl","maxTokens","temperature","loadConfig","savedConfig","localStorage","getItem","JSON","parse","error","console","saveConfig","configToSave","setItem","stringify","updateConfig","newConfig","getConfig","isConfigured","trim","length","generateAnalysis","type","originalText","changedText","Error","prompt","createPrompt","response","callLLM","formatResponse","message","baseContext","callOpenAI","callAnthropic","callOllama","callGemini","fetch","method","headers","body","messages","role","content","max_tokens","ok","_error$error","json","status","data","choices","_error$error2","text","ollamaUrl","stream","options","num_predict","contents","parts","generationConfig","maxOutputTokens","_error$error3","candidates","typeIcons","explain","rewrite","summary","tone","cleanup","typeTitles","title","testConnection","testPrompt","success","getAvailableModels"],"sources":["C:/Users/Joem/quickdiff-react/src/utils/LLMService.js"],"sourcesContent":["/**\r\n * LLM Service for QuickDiff React App\r\n * Supports multiple LLM providers: OpenAI, Anthropic, Ollama (local), and Gemini\r\n */\r\n\r\nexport class LLMService {\r\n  constructor() {\r\n    this.config = {\r\n      provider: 'openai', // 'openai', 'anthropic', 'ollama', 'gemini'\r\n      apiKey: '', // Set via environment or user input\r\n      model: 'gpt-3.5-turbo', // Default model\r\n      baseUrl: 'https://api.openai.com/v1', // Can be changed for local/custom endpoints\r\n      maxTokens: 1000,\r\n      temperature: 0.7\r\n    };\r\n    \r\n    // Load config from localStorage if available\r\n    this.loadConfig();\r\n  }\r\n\r\n  loadConfig() {\r\n    try {\r\n      const savedConfig = localStorage.getItem('quickdiff_llm_config');\r\n      if (savedConfig) {\r\n        this.config = { ...this.config, ...JSON.parse(savedConfig) };\r\n      }\r\n    } catch (error) {\r\n      console.error('Error loading LLM config:', error);\r\n    }\r\n  }\r\n\r\n  saveConfig() {\r\n    try {\r\n      // Don't save API key to localStorage for security\r\n      const configToSave = { ...this.config };\r\n      delete configToSave.apiKey;\r\n      localStorage.setItem('quickdiff_llm_config', JSON.stringify(configToSave));\r\n    } catch (error) {\r\n      console.error('Error saving LLM config:', error);\r\n    }\r\n  }\r\n\r\n  updateConfig(newConfig) {\r\n    this.config = { ...this.config, ...newConfig };\r\n    this.saveConfig();\r\n  }\r\n\r\n  getConfig() {\r\n    return { ...this.config };\r\n  }\r\n\r\n  // Check if LLM is properly configured\r\n  isConfigured() {\r\n    if (this.config.provider === 'ollama') {\r\n      return true; // Ollama doesn't require API key\r\n    }\r\n    return this.config.apiKey && this.config.apiKey.trim().length > 0;\r\n  }\r\n\r\n  // Generate analysis using the configured LLM\r\n  async generateAnalysis(type, originalText, changedText) {\r\n    if (!this.isConfigured()) {\r\n      throw new Error('LLM not configured. Please set up your API key and provider.');\r\n    }\r\n\r\n    const prompt = this.createPrompt(type, originalText, changedText);\r\n    \r\n    try {\r\n      const response = await this.callLLM(prompt);\r\n      return this.formatResponse(type, response);\r\n    } catch (error) {\r\n      console.error('LLM API Error:', error);\r\n      throw new Error(`Failed to generate ${type} analysis: ${error.message}`);\r\n    }\r\n  }\r\n\r\n  // Create appropriate prompt based on analysis type\r\n  createPrompt(type, originalText, changedText) {\r\n    const baseContext = `You are analyzing text differences. Here are the texts:\r\n\r\nORIGINAL TEXT:\r\n${originalText || '(empty)'}\r\n\r\nCHANGED TEXT:\r\n${changedText || '(empty)'}\r\n\r\n`;\r\n\r\n    switch (type) {\r\n      case 'explain':\r\n        return baseContext + `Please provide a detailed explanation of the differences between these texts. Focus on:\r\n1. What specific changes were made\r\n2. The significance of these changes\r\n3. Potential impact or implications\r\n4. Key statistics (word count, line count changes)\r\n\r\nFormat your response in HTML with appropriate headings and structure.`;\r\n\r\n      case 'rewrite':\r\n        return baseContext + `Please provide specific rewrite suggestions to improve the text. Focus on:\r\n1. Style and clarity improvements\r\n2. Grammar and syntax corrections\r\n3. Structure and organization suggestions\r\n4. Tone and readability enhancements\r\n\r\nFormat your response in HTML with specific, actionable suggestions.`;\r\n\r\n      case 'summary':\r\n        return baseContext + `Please provide a concise summary of both texts, highlighting:\r\n1. Main topics and themes\r\n2. Key differences between versions\r\n3. Important information preserved or lost\r\n4. Overall content assessment\r\n\r\nFormat your response in HTML with clear sections.`;\r\n\r\n      case 'tone':\r\n        return baseContext + `Please analyze the tone and style of both texts. Include:\r\n1. Sentiment analysis (positive, negative, neutral)\r\n2. Formality level assessment\r\n3. Writing style characteristics\r\n4. Tone changes between versions\r\n5. Recommendations for tone consistency\r\n\r\nFormat your response in HTML with detailed analysis.`;\r\n\r\n      case 'cleanup':\r\n        return baseContext + `Please analyze the text for formatting and cleanup issues. Identify:\r\n1. Formatting inconsistencies\r\n2. Punctuation and spacing issues\r\n3. Structural problems\r\n4. Specific cleanup recommendations\r\n5. Best practices for improvement\r\n\r\nFormat your response in HTML with actionable cleanup suggestions.`;\r\n\r\n      default:\r\n        return baseContext + `Please analyze these texts and provide insights about their differences and characteristics. Format your response in HTML.`;\r\n    }\r\n  }\r\n\r\n  // Call the appropriate LLM API based on provider\r\n  async callLLM(prompt) {\r\n    switch (this.config.provider) {\r\n      case 'openai':\r\n        return await this.callOpenAI(prompt);\r\n      case 'anthropic':\r\n        return await this.callAnthropic(prompt);\r\n      case 'ollama':\r\n        return await this.callOllama(prompt);\r\n      case 'gemini':\r\n        return await this.callGemini(prompt);\r\n      default:\r\n        throw new Error(`Unsupported LLM provider: ${this.config.provider}`);\r\n    }\r\n  }\r\n\r\n  // OpenAI API call\r\n  async callOpenAI(prompt) {\r\n    const response = await fetch(`${this.config.baseUrl}/chat/completions`, {\r\n      method: 'POST',\r\n      headers: {\r\n        'Content-Type': 'application/json',\r\n        'Authorization': `Bearer ${this.config.apiKey}`\r\n      },\r\n      body: JSON.stringify({\r\n        model: this.config.model,\r\n        messages: [\r\n          {\r\n            role: 'system',\r\n            content: 'You are a helpful assistant that analyzes text differences and provides detailed insights.'\r\n          },\r\n          {\r\n            role: 'user',\r\n            content: prompt\r\n          }\r\n        ],\r\n        max_tokens: this.config.maxTokens,\r\n        temperature: this.config.temperature\r\n      })\r\n    });\r\n\r\n    if (!response.ok) {\r\n      const error = await response.json();\r\n      throw new Error(error.error?.message || `HTTP ${response.status}`);\r\n    }\r\n\r\n    const data = await response.json();\r\n    return data.choices[0].message.content;\r\n  }\r\n\r\n  // Anthropic Claude API call\r\n  async callAnthropic(prompt) {\r\n    const response = await fetch('https://api.anthropic.com/v1/messages', {\r\n      method: 'POST',\r\n      headers: {\r\n        'Content-Type': 'application/json',\r\n        'x-api-key': this.config.apiKey,\r\n        'anthropic-version': '2023-06-01'\r\n      },\r\n      body: JSON.stringify({\r\n        model: this.config.model || 'claude-3-sonnet-20240229',\r\n        max_tokens: this.config.maxTokens,\r\n        messages: [\r\n          {\r\n            role: 'user',\r\n            content: prompt\r\n          }\r\n        ]\r\n      })\r\n    });\r\n\r\n    if (!response.ok) {\r\n      const error = await response.json();\r\n      throw new Error(error.error?.message || `HTTP ${response.status}`);\r\n    }\r\n\r\n    const data = await response.json();\r\n    return data.content[0].text;\r\n  }\r\n\r\n  // Ollama local API call\r\n  async callOllama(prompt) {\r\n    const ollamaUrl = this.config.baseUrl || 'http://localhost:11434';\r\n    const response = await fetch(`${ollamaUrl}/api/generate`, {\r\n      method: 'POST',\r\n      headers: {\r\n        'Content-Type': 'application/json'\r\n      },\r\n      body: JSON.stringify({\r\n        model: this.config.model || 'llama2',\r\n        prompt: prompt,\r\n        stream: false,\r\n        options: {\r\n          temperature: this.config.temperature,\r\n          num_predict: this.config.maxTokens\r\n        }\r\n      })\r\n    });\r\n\r\n    if (!response.ok) {\r\n      throw new Error(`Ollama API error: HTTP ${response.status}`);\r\n    }\r\n\r\n    const data = await response.json();\r\n    return data.response;\r\n  }\r\n\r\n  // Google Gemini API call\r\n  async callGemini(prompt) {\r\n    const response = await fetch(`https://generativelanguage.googleapis.com/v1beta/models/${this.config.model || 'gemini-pro'}:generateContent?key=${this.config.apiKey}`, {\r\n      method: 'POST',\r\n      headers: {\r\n        'Content-Type': 'application/json'\r\n      },\r\n      body: JSON.stringify({\r\n        contents: [\r\n          {\r\n            parts: [\r\n              {\r\n                text: prompt\r\n              }\r\n            ]\r\n          }\r\n        ],\r\n        generationConfig: {\r\n          temperature: this.config.temperature,\r\n          maxOutputTokens: this.config.maxTokens\r\n        }\r\n      })\r\n    });\r\n\r\n    if (!response.ok) {\r\n      const error = await response.json();\r\n      throw new Error(error.error?.message || `HTTP ${response.status}`);\r\n    }\r\n\r\n    const data = await response.json();\r\n    return data.candidates[0].content.parts[0].text;\r\n  }\r\n\r\n  // Format the LLM response into the expected structure\r\n  formatResponse(type, content) {\r\n    const typeIcons = {\r\n      explain: 'üß†',\r\n      rewrite: '‚ú®',\r\n      summary: 'üìù',\r\n      tone: 'üé≠',\r\n      cleanup: 'üßπ'\r\n    };\r\n\r\n    const typeTitles = {\r\n      explain: 'AI Explanation',\r\n      rewrite: 'AI Rewrite Suggestions',\r\n      summary: 'AI Summary',\r\n      tone: 'AI Tone Analysis',\r\n      cleanup: 'AI Text Cleanup'\r\n    };\r\n\r\n    return {\r\n      title: `${typeIcons[type] || 'ü§ñ'} ${typeTitles[type] || 'AI Analysis'}`,\r\n      content: `<div class=\"ai-${type}\">${content}</div>`,\r\n      type: type\r\n    };\r\n  }\r\n\r\n  // Test the LLM connection\r\n  async testConnection() {\r\n    try {\r\n      const testPrompt = \"Please respond with 'Connection successful!' to test the API.\";\r\n      const response = await this.callLLM(testPrompt);\r\n      return { success: true, response };\r\n    } catch (error) {\r\n      return { success: false, error: error.message };\r\n    }\r\n  }\r\n\r\n  // Get available models for the current provider\r\n  getAvailableModels() {\r\n    switch (this.config.provider) {\r\n      case 'openai':\r\n        return [\r\n          'gpt-4',\r\n          'gpt-4-turbo-preview',\r\n          'gpt-3.5-turbo',\r\n          'gpt-3.5-turbo-16k'\r\n        ];\r\n      case 'anthropic':\r\n        return [\r\n          'claude-3-opus-20240229',\r\n          'claude-3-sonnet-20240229',\r\n          'claude-3-haiku-20240307'\r\n        ];\r\n      case 'ollama':\r\n        return [\r\n          'llama2',\r\n          'llama2:13b',\r\n          'llama2:70b',\r\n          'codellama',\r\n          'mistral',\r\n          'neural-chat'\r\n        ];\r\n      case 'gemini':\r\n        return [\r\n          'gemini-pro',\r\n          'gemini-pro-vision'\r\n        ];\r\n      default:\r\n        return [];\r\n    }\r\n  }\r\n}"],"mappings":"AAAA;AACA;AACA;AACA;;AAEA,OAAO,MAAMA,UAAU,CAAC;EACtBC,WAAWA,CAAA,EAAG;IACZ,IAAI,CAACC,MAAM,GAAG;MACZC,QAAQ,EAAE,QAAQ;MAAE;MACpBC,MAAM,EAAE,EAAE;MAAE;MACZC,KAAK,EAAE,eAAe;MAAE;MACxBC,OAAO,EAAE,2BAA2B;MAAE;MACtCC,SAAS,EAAE,IAAI;MACfC,WAAW,EAAE;IACf,CAAC;;IAED;IACA,IAAI,CAACC,UAAU,CAAC,CAAC;EACnB;EAEAA,UAAUA,CAAA,EAAG;IACX,IAAI;MACF,MAAMC,WAAW,GAAGC,YAAY,CAACC,OAAO,CAAC,sBAAsB,CAAC;MAChE,IAAIF,WAAW,EAAE;QACf,IAAI,CAACR,MAAM,GAAG;UAAE,GAAG,IAAI,CAACA,MAAM;UAAE,GAAGW,IAAI,CAACC,KAAK,CAACJ,WAAW;QAAE,CAAC;MAC9D;IACF,CAAC,CAAC,OAAOK,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,2BAA2B,EAAEA,KAAK,CAAC;IACnD;EACF;EAEAE,UAAUA,CAAA,EAAG;IACX,IAAI;MACF;MACA,MAAMC,YAAY,GAAG;QAAE,GAAG,IAAI,CAAChB;MAAO,CAAC;MACvC,OAAOgB,YAAY,CAACd,MAAM;MAC1BO,YAAY,CAACQ,OAAO,CAAC,sBAAsB,EAAEN,IAAI,CAACO,SAAS,CAACF,YAAY,CAAC,CAAC;IAC5E,CAAC,CAAC,OAAOH,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,0BAA0B,EAAEA,KAAK,CAAC;IAClD;EACF;EAEAM,YAAYA,CAACC,SAAS,EAAE;IACtB,IAAI,CAACpB,MAAM,GAAG;MAAE,GAAG,IAAI,CAACA,MAAM;MAAE,GAAGoB;IAAU,CAAC;IAC9C,IAAI,CAACL,UAAU,CAAC,CAAC;EACnB;EAEAM,SAASA,CAAA,EAAG;IACV,OAAO;MAAE,GAAG,IAAI,CAACrB;IAAO,CAAC;EAC3B;;EAEA;EACAsB,YAAYA,CAAA,EAAG;IACb,IAAI,IAAI,CAACtB,MAAM,CAACC,QAAQ,KAAK,QAAQ,EAAE;MACrC,OAAO,IAAI,CAAC,CAAC;IACf;IACA,OAAO,IAAI,CAACD,MAAM,CAACE,MAAM,IAAI,IAAI,CAACF,MAAM,CAACE,MAAM,CAACqB,IAAI,CAAC,CAAC,CAACC,MAAM,GAAG,CAAC;EACnE;;EAEA;EACA,MAAMC,gBAAgBA,CAACC,IAAI,EAAEC,YAAY,EAAEC,WAAW,EAAE;IACtD,IAAI,CAAC,IAAI,CAACN,YAAY,CAAC,CAAC,EAAE;MACxB,MAAM,IAAIO,KAAK,CAAC,8DAA8D,CAAC;IACjF;IAEA,MAAMC,MAAM,GAAG,IAAI,CAACC,YAAY,CAACL,IAAI,EAAEC,YAAY,EAAEC,WAAW,CAAC;IAEjE,IAAI;MACF,MAAMI,QAAQ,GAAG,MAAM,IAAI,CAACC,OAAO,CAACH,MAAM,CAAC;MAC3C,OAAO,IAAI,CAACI,cAAc,CAACR,IAAI,EAAEM,QAAQ,CAAC;IAC5C,CAAC,CAAC,OAAOnB,KAAK,EAAE;MACdC,OAAO,CAACD,KAAK,CAAC,gBAAgB,EAAEA,KAAK,CAAC;MACtC,MAAM,IAAIgB,KAAK,CAAC,sBAAsBH,IAAI,cAAcb,KAAK,CAACsB,OAAO,EAAE,CAAC;IAC1E;EACF;;EAEA;EACAJ,YAAYA,CAACL,IAAI,EAAEC,YAAY,EAAEC,WAAW,EAAE;IAC5C,MAAMQ,WAAW,GAAG;AACxB;AACA;AACA,EAAET,YAAY,IAAI,SAAS;AAC3B;AACA;AACA,EAAEC,WAAW,IAAI,SAAS;AAC1B;AACA,CAAC;IAEG,QAAQF,IAAI;MACV,KAAK,SAAS;QACZ,OAAOU,WAAW,GAAG;AAC7B;AACA;AACA;AACA;AACA;AACA,sEAAsE;MAEhE,KAAK,SAAS;QACZ,OAAOA,WAAW,GAAG;AAC7B;AACA;AACA;AACA;AACA;AACA,oEAAoE;MAE9D,KAAK,SAAS;QACZ,OAAOA,WAAW,GAAG;AAC7B;AACA;AACA;AACA;AACA;AACA,kDAAkD;MAE5C,KAAK,MAAM;QACT,OAAOA,WAAW,GAAG;AAC7B;AACA;AACA;AACA;AACA;AACA;AACA,qDAAqD;MAE/C,KAAK,SAAS;QACZ,OAAOA,WAAW,GAAG;AAC7B;AACA;AACA;AACA;AACA;AACA;AACA,kEAAkE;MAE5D;QACE,OAAOA,WAAW,GAAG,4HAA4H;IACrJ;EACF;;EAEA;EACA,MAAMH,OAAOA,CAACH,MAAM,EAAE;IACpB,QAAQ,IAAI,CAAC9B,MAAM,CAACC,QAAQ;MAC1B,KAAK,QAAQ;QACX,OAAO,MAAM,IAAI,CAACoC,UAAU,CAACP,MAAM,CAAC;MACtC,KAAK,WAAW;QACd,OAAO,MAAM,IAAI,CAACQ,aAAa,CAACR,MAAM,CAAC;MACzC,KAAK,QAAQ;QACX,OAAO,MAAM,IAAI,CAACS,UAAU,CAACT,MAAM,CAAC;MACtC,KAAK,QAAQ;QACX,OAAO,MAAM,IAAI,CAACU,UAAU,CAACV,MAAM,CAAC;MACtC;QACE,MAAM,IAAID,KAAK,CAAC,6BAA6B,IAAI,CAAC7B,MAAM,CAACC,QAAQ,EAAE,CAAC;IACxE;EACF;;EAEA;EACA,MAAMoC,UAAUA,CAACP,MAAM,EAAE;IACvB,MAAME,QAAQ,GAAG,MAAMS,KAAK,CAAC,GAAG,IAAI,CAACzC,MAAM,CAACI,OAAO,mBAAmB,EAAE;MACtEsC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,eAAe,EAAE,UAAU,IAAI,CAAC3C,MAAM,CAACE,MAAM;MAC/C,CAAC;MACD0C,IAAI,EAAEjC,IAAI,CAACO,SAAS,CAAC;QACnBf,KAAK,EAAE,IAAI,CAACH,MAAM,CAACG,KAAK;QACxB0C,QAAQ,EAAE,CACR;UACEC,IAAI,EAAE,QAAQ;UACdC,OAAO,EAAE;QACX,CAAC,EACD;UACED,IAAI,EAAE,MAAM;UACZC,OAAO,EAAEjB;QACX,CAAC,CACF;QACDkB,UAAU,EAAE,IAAI,CAAChD,MAAM,CAACK,SAAS;QACjCC,WAAW,EAAE,IAAI,CAACN,MAAM,CAACM;MAC3B,CAAC;IACH,CAAC,CAAC;IAEF,IAAI,CAAC0B,QAAQ,CAACiB,EAAE,EAAE;MAAA,IAAAC,YAAA;MAChB,MAAMrC,KAAK,GAAG,MAAMmB,QAAQ,CAACmB,IAAI,CAAC,CAAC;MACnC,MAAM,IAAItB,KAAK,CAAC,EAAAqB,YAAA,GAAArC,KAAK,CAACA,KAAK,cAAAqC,YAAA,uBAAXA,YAAA,CAAaf,OAAO,KAAI,QAAQH,QAAQ,CAACoB,MAAM,EAAE,CAAC;IACpE;IAEA,MAAMC,IAAI,GAAG,MAAMrB,QAAQ,CAACmB,IAAI,CAAC,CAAC;IAClC,OAAOE,IAAI,CAACC,OAAO,CAAC,CAAC,CAAC,CAACnB,OAAO,CAACY,OAAO;EACxC;;EAEA;EACA,MAAMT,aAAaA,CAACR,MAAM,EAAE;IAC1B,MAAME,QAAQ,GAAG,MAAMS,KAAK,CAAC,uCAAuC,EAAE;MACpEC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE,kBAAkB;QAClC,WAAW,EAAE,IAAI,CAAC3C,MAAM,CAACE,MAAM;QAC/B,mBAAmB,EAAE;MACvB,CAAC;MACD0C,IAAI,EAAEjC,IAAI,CAACO,SAAS,CAAC;QACnBf,KAAK,EAAE,IAAI,CAACH,MAAM,CAACG,KAAK,IAAI,0BAA0B;QACtD6C,UAAU,EAAE,IAAI,CAAChD,MAAM,CAACK,SAAS;QACjCwC,QAAQ,EAAE,CACR;UACEC,IAAI,EAAE,MAAM;UACZC,OAAO,EAAEjB;QACX,CAAC;MAEL,CAAC;IACH,CAAC,CAAC;IAEF,IAAI,CAACE,QAAQ,CAACiB,EAAE,EAAE;MAAA,IAAAM,aAAA;MAChB,MAAM1C,KAAK,GAAG,MAAMmB,QAAQ,CAACmB,IAAI,CAAC,CAAC;MACnC,MAAM,IAAItB,KAAK,CAAC,EAAA0B,aAAA,GAAA1C,KAAK,CAACA,KAAK,cAAA0C,aAAA,uBAAXA,aAAA,CAAapB,OAAO,KAAI,QAAQH,QAAQ,CAACoB,MAAM,EAAE,CAAC;IACpE;IAEA,MAAMC,IAAI,GAAG,MAAMrB,QAAQ,CAACmB,IAAI,CAAC,CAAC;IAClC,OAAOE,IAAI,CAACN,OAAO,CAAC,CAAC,CAAC,CAACS,IAAI;EAC7B;;EAEA;EACA,MAAMjB,UAAUA,CAACT,MAAM,EAAE;IACvB,MAAM2B,SAAS,GAAG,IAAI,CAACzD,MAAM,CAACI,OAAO,IAAI,wBAAwB;IACjE,MAAM4B,QAAQ,GAAG,MAAMS,KAAK,CAAC,GAAGgB,SAAS,eAAe,EAAE;MACxDf,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEjC,IAAI,CAACO,SAAS,CAAC;QACnBf,KAAK,EAAE,IAAI,CAACH,MAAM,CAACG,KAAK,IAAI,QAAQ;QACpC2B,MAAM,EAAEA,MAAM;QACd4B,MAAM,EAAE,KAAK;QACbC,OAAO,EAAE;UACPrD,WAAW,EAAE,IAAI,CAACN,MAAM,CAACM,WAAW;UACpCsD,WAAW,EAAE,IAAI,CAAC5D,MAAM,CAACK;QAC3B;MACF,CAAC;IACH,CAAC,CAAC;IAEF,IAAI,CAAC2B,QAAQ,CAACiB,EAAE,EAAE;MAChB,MAAM,IAAIpB,KAAK,CAAC,0BAA0BG,QAAQ,CAACoB,MAAM,EAAE,CAAC;IAC9D;IAEA,MAAMC,IAAI,GAAG,MAAMrB,QAAQ,CAACmB,IAAI,CAAC,CAAC;IAClC,OAAOE,IAAI,CAACrB,QAAQ;EACtB;;EAEA;EACA,MAAMQ,UAAUA,CAACV,MAAM,EAAE;IACvB,MAAME,QAAQ,GAAG,MAAMS,KAAK,CAAC,2DAA2D,IAAI,CAACzC,MAAM,CAACG,KAAK,IAAI,YAAY,wBAAwB,IAAI,CAACH,MAAM,CAACE,MAAM,EAAE,EAAE;MACrKwC,MAAM,EAAE,MAAM;MACdC,OAAO,EAAE;QACP,cAAc,EAAE;MAClB,CAAC;MACDC,IAAI,EAAEjC,IAAI,CAACO,SAAS,CAAC;QACnB2C,QAAQ,EAAE,CACR;UACEC,KAAK,EAAE,CACL;YACEN,IAAI,EAAE1B;UACR,CAAC;QAEL,CAAC,CACF;QACDiC,gBAAgB,EAAE;UAChBzD,WAAW,EAAE,IAAI,CAACN,MAAM,CAACM,WAAW;UACpC0D,eAAe,EAAE,IAAI,CAAChE,MAAM,CAACK;QAC/B;MACF,CAAC;IACH,CAAC,CAAC;IAEF,IAAI,CAAC2B,QAAQ,CAACiB,EAAE,EAAE;MAAA,IAAAgB,aAAA;MAChB,MAAMpD,KAAK,GAAG,MAAMmB,QAAQ,CAACmB,IAAI,CAAC,CAAC;MACnC,MAAM,IAAItB,KAAK,CAAC,EAAAoC,aAAA,GAAApD,KAAK,CAACA,KAAK,cAAAoD,aAAA,uBAAXA,aAAA,CAAa9B,OAAO,KAAI,QAAQH,QAAQ,CAACoB,MAAM,EAAE,CAAC;IACpE;IAEA,MAAMC,IAAI,GAAG,MAAMrB,QAAQ,CAACmB,IAAI,CAAC,CAAC;IAClC,OAAOE,IAAI,CAACa,UAAU,CAAC,CAAC,CAAC,CAACnB,OAAO,CAACe,KAAK,CAAC,CAAC,CAAC,CAACN,IAAI;EACjD;;EAEA;EACAtB,cAAcA,CAACR,IAAI,EAAEqB,OAAO,EAAE;IAC5B,MAAMoB,SAAS,GAAG;MAChBC,OAAO,EAAE,IAAI;MACbC,OAAO,EAAE,GAAG;MACZC,OAAO,EAAE,IAAI;MACbC,IAAI,EAAE,IAAI;MACVC,OAAO,EAAE;IACX,CAAC;IAED,MAAMC,UAAU,GAAG;MACjBL,OAAO,EAAE,gBAAgB;MACzBC,OAAO,EAAE,wBAAwB;MACjCC,OAAO,EAAE,YAAY;MACrBC,IAAI,EAAE,kBAAkB;MACxBC,OAAO,EAAE;IACX,CAAC;IAED,OAAO;MACLE,KAAK,EAAE,GAAGP,SAAS,CAACzC,IAAI,CAAC,IAAI,IAAI,IAAI+C,UAAU,CAAC/C,IAAI,CAAC,IAAI,aAAa,EAAE;MACxEqB,OAAO,EAAE,kBAAkBrB,IAAI,KAAKqB,OAAO,QAAQ;MACnDrB,IAAI,EAAEA;IACR,CAAC;EACH;;EAEA;EACA,MAAMiD,cAAcA,CAAA,EAAG;IACrB,IAAI;MACF,MAAMC,UAAU,GAAG,+DAA+D;MAClF,MAAM5C,QAAQ,GAAG,MAAM,IAAI,CAACC,OAAO,CAAC2C,UAAU,CAAC;MAC/C,OAAO;QAAEC,OAAO,EAAE,IAAI;QAAE7C;MAAS,CAAC;IACpC,CAAC,CAAC,OAAOnB,KAAK,EAAE;MACd,OAAO;QAAEgE,OAAO,EAAE,KAAK;QAAEhE,KAAK,EAAEA,KAAK,CAACsB;MAAQ,CAAC;IACjD;EACF;;EAEA;EACA2C,kBAAkBA,CAAA,EAAG;IACnB,QAAQ,IAAI,CAAC9E,MAAM,CAACC,QAAQ;MAC1B,KAAK,QAAQ;QACX,OAAO,CACL,OAAO,EACP,qBAAqB,EACrB,eAAe,EACf,mBAAmB,CACpB;MACH,KAAK,WAAW;QACd,OAAO,CACL,wBAAwB,EACxB,0BAA0B,EAC1B,yBAAyB,CAC1B;MACH,KAAK,QAAQ;QACX,OAAO,CACL,QAAQ,EACR,YAAY,EACZ,YAAY,EACZ,WAAW,EACX,SAAS,EACT,aAAa,CACd;MACH,KAAK,QAAQ;QACX,OAAO,CACL,YAAY,EACZ,mBAAmB,CACpB;MACH;QACE,OAAO,EAAE;IACb;EACF;AACF","ignoreList":[]},"metadata":{},"sourceType":"module","externalDependencies":[]}